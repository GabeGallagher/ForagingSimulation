- It's weird that normal mathematical convention considers the primary axis to be the x-axis, while robotics thinks in terms of directions (north, east, south, west) so the primary axis becomes the y-axis. This makes sense, because robotics deals with bodies that move through the world, so they need a relative perspective which will always consider forward to be the primary point of referece. Math, I supposes, is fundamentally read left to right, therefore it makes sense that the x-axis would be the primary. I've never thought about fundamental orientation in this way before, but it keeps coming up when using robotics because many of the math libraries write coordinates in x, y, while in robotics it's flipped to y, x. I'll have to keep track of this in my head and I forsee some stupid bugs arising from this. I wonder if there are other aspects of robotics that are at odds with mathematics in this way. If so, I wonder if there are libraries that handle these issues, or if it would be worth it to write my own.
- I'm bad at the mathematics of orientations. Might need to review some trig
- How do we handle movement? I was thinking that this would actually be a pretty trivial part of the simulation, but I ran into an interesting question. volicity is measured as direction * movement along the x/y axes over time. However, where does that over time come from? Is the nest aware of the time, and constantly send signals to the bots to move at a given time step? Should the bots be aware of the time and their movement speed? The solution I came up with is to use a similar solution to what I've worked with in game engines and use an update loop that's tightly coupled with real time, but allows different object to only use time and time changes as needed. The nest likely needs to know what time it is for more advanced procedures, but separating the time from the brain forces the constraint of the brain working within time constraints, rather than giving the ability to manipulate time for more effecient solutions. Of course, the bots don't need to be aware of time, just that they are moving at a given speed.
Feels like the most difficult part of this so far has been writing the world time and animation code. Makes me think using an engine like unity or godot would be better for the simulation, while maintaining code to run headless
Another issue is keeping track of what different objects are in python, since it is not a strongly typed language. I only forsee this becoming more of an issue as this project gets bigger. Wonder what way there are to help mitigate this.
Also, is VS code, imports are kind of annoying. I'm wondering if there are vs code packages for python that make it easier to infer imports like what pycharm does

I'm finally implementing navigation algorithms, which is exciting and daunting at the same time. I'm already seeing that I will need to increment the complexity, start with a single bot, a single obstacle, and a single target, and work up to more of each
It appears that different algorithms are better in different scenarios. For example, A* will be effecient when I know the whole arena and can write a different algorithm to reduce it to a grid in the eyes of the brain. Potential field struggles with local minima. There are probably other algorithms that are good for exploring an unkown arena, and then I will want to use a different algorithm to determine if my bots understand the arena well enough to switch to an algorithm that is good for a fully understand arena
In working with the potential field algorithm, What will happen when the field is dynamic? For example, if I introduce more bots that are moving?
There are a couple of constants in the formulae for the potential field algorithm but it is unclear how to find those constants for a given arena. I assume they will need trial, simulation, and iteration
Now that I'm starting actually implementing the potential field algo, one thing I've immediately noticed is that I have to reconsider how I handle collision with the borders of the arena and the obstacles within. Potential field doesn't actually use collision, but it will require a check each frame to see if the bot is at the influence zone of the colliders and how to react. With many bots, this could be a lot of unnecessary computations. In a larger sim, I'll have to refactor the collision in such a way that I can pass the influence zone to the different colliders and then trigger events while within that zone. For the current implementation, I can just have the nest handle all of the calculations each frame since the sim isn't large enough yet for this to be an issue. I don't think it will be an issue either since this is just for learning, but will need to be considered for more robust arenas